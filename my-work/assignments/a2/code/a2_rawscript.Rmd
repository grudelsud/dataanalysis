2nd Assignment
========================================================

todo
--------------------------------------------------------
Your task is to build a function that predicts what activity a subject is performing based on the quantitative measurements from the Samsung phone. For this analysis your training set must include the data from subjects 1, 3, 5, and 6.  But you may use more subjects data to train if you wish. Your test set is the data from subjects 27, 28, 29, and 30, but you may use more data to test. Be careful that your training/test sets do not overlap. 

introduction
--------------------------------------------------------
Research in activity and gesture recognition is quite important nowadays [ref. 3], as it can help in several different fields, such as video surveillance, health, disease prevention and much more. Since the introduction of smartphones, and most importantly the accelerometers mounted in the devices, along with their wide adoption across all countries, plenty of data describing different gestures has become availble. This data can be used proficiently to monitor and predict activities of people carrying the device, to help them expoiting the goals fixed by a specific application that takes advantage of these values.
In this paper we will show how a set of data can be used to train models and predict what activity is performed by a user producing comparable values through an accelerometer mounted on a mobile device.

data collection
--------------------------------------------------------
The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING, WALKING UPSTAIRS, WALKING DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope, we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers was selected for generating the training data and 30% the test data. [ref. 1]

All of the columns of the data set (except the last two) represents one measurement from the Samsung phone. The variable subject indicates which subject was performing the tasks when the measurements were taken. The variable activity tells what activity they were performing. [ref. 2]

```{r}

# uncomment to download it again
# data_url <- 'https://spark-public.s3.amazonaws.com/dataanalysis/samsungData.rda'
# download.file(data_url, '../data/samsungData.rda', method='curl')

# show date for the records
# download_when <- date()
# print(download_when)

# the file available here was downloaded on Fri Mar 8th, at 11:16 AM
load('../data/samsungData.rda')

```

exploratory analysis
--------------------------------------------------------
Great help has been provided by the community in the initial approach to understanding how the acceleramoter data is structured, through a discussion around differences between total body acceleration, gravity, jerk and all related elements [ref. 4].

```{r}
names(samsungData)[1:40]
names(samsungData)[41:80]
names(samsungData)[81:120]

table(samsungData$activity)
table(samsungData$subject)
```

* 1st 40 columns contain data related to body acceleration along the 3 axes, this is taking into account the gravity vector
* columns 41-80 contain data related to gravity acceleration only along the 3 axes
* columns 81-120 contain data related to acceleration jerk for the body along the 3 axes (jerk is 3rd order derivative of position, equals 2nd order derivative of velocity, 1st order derivative of acceleration [ref. 5])
* further exploration shows that: columns 121-160 contain body gyroscope, 161-200 body gyroscope jerk, and up to column 265 other derivatives in the time domain, while from column 266 to column 554 the values transformed in the frequency domain, then columns 556 to 561 for the angles between accelerometer vectors and the last two columns dedicated to subject and activity labelings
* table of activities show how they are equally distributed across 6 items: laying, sitting, standing, walk, walkdown, walkup
* table of subjects show how they are equally distributed across 30 people, for the purpose of this analysis, we are going to use subjects 1, 3, 5, 6 for training sets and subjects 27, 28, 29, and 30 for tests.

```{r}
summary(samsungData[1:40])
boxplot(samsungData[1:40])
```

a quick observation of the 1st 40 columns reveals that there are not empty values in the dataset, and a boxplot shows that they are all normalized in a [-1,1] interval, so it should not need removal of outliers. the same applies the all numeric values contained in the dataset.
summary also reveals that funny names are used for variables, to stay on a safe side, when creating the training and test set, they will be converted using R function make.names.


```{r}
class(samsungData$activity)
samsungData$activity 
```

activity field reveals it has been stored as character, we will leave it untouched in the source data, but will be converted to factor in thet test set.

```{r}
# transforming a bit the samsungData so as to cleanup names
samsung_procdata <- data.frame(samsungData, check.names=TRUE)
samsung_procdata$activity <- as.factor(samsung_procdata$activity)

# preparing the training set
training_subjects <- c(1,3,5,6)
training_set <- subset(samsung_procdata, samsung_procdata$subject==training_subjects)
training_set <- training_set[order(training_set$activity),]
training_set$subject <- NULL

factor_activity <- unique(training_set$activity)

# pick a few subjects for validation set
validation_subjects <- c(14,15,16,17)
validation_set <- subset(samsung_procdata, samsung_procdata$subject==validation_subjects)
validation_set <- validation_set[order(validation_set$activity),]
validation_set$subject <- NULL

# doing the same for test set
test_subjects <- c(27,28,29,30)
test_set <- subset(samsung_procdata, samsung_procdata$subject==test_subjects)
test_set <- test_set[order(test_set$activity),]
test_set$subject <- NULL

```

we subset the whole dataframe for training, validation and tests with the subjects we want to use, then ordering by activity, cleaning up variable names and removing the column related to subjects since we don't want it to appear in the model. we can see that the training set is made of 328 obs., validation 346 obs. and test 376 obs., while the training set used for prediction trees trained without bootstrap, as described below, will contain 5867 observations. all the observations are consistently made of 562 variables.

statistical modeling
--------------------------------------------------------

we are going to use the data from subjects 1,3,5,6 as training sets. this data will be used to create several regression models using a mixture kernel based functions (i.e. using randomForests and support vector machines) taking extra care in balancing the bias / variance trade off in statistical modeling, as described in the analysis section of this paper. particularly, given the nature of this dataset, differences in errors will be evaulated when using bootstrap techniques in favour of enlarging the dataset and creating a tree model without sampling. A complete recap of statistical methods (regression models and bootstrap technique) mentioned in this paper can be found in [ref. 6]

analysis
--------------------------------------------------------

we set a seed so as to generate a consistent set of training models, so that errors on validation and test will be always consistent.

```{r}
library(randomForest)
library(e1071)

set.seed(987654321)
mod_randomforest <- randomForest(activity ~ ., data=training_set, prox=TRUE)
mod_svm <- svm(activity ~ ., data=training_set)

print(mod_randomforest)
print(mod_svm)
```

random forest showing error < 4% in activity classification, now we can do some prediction on the validation test to see how the models behave.

```{r}
predvalid_randomforest <- predict(mod_randomforest, validation_set)
predvalid_svm <- predict(mod_svm, validation_set)

errvalid_randomforest <- sum(predvalid_randomforest != validation_set$activity) / length(validation_set$activity)
print(errvalid_randomforest)

errvalid_svm <- sum(predvalid_svm != validation_set$activity) / length(validation_set$activity)
print(errvalid_svm)

```

17% error using random forst: given that a random forest training is natively executed using boostrap technique, it seems that the model is not overfitting the data, returning a reasonable error for a correct classification but still allowing a little bias in the data. while we obtain 25% error using svm, which doesn't look encouraging on the training set: this will be left as is, to show how bootstrap sensibly increases the accuracy of prediction.

to compare the error of a random forest with a model without bootstrap, we enlarge the training set using a greater number of subjects (in this case we have a training set made of 5867 observations of 562 variables) then we use normal decision trees on the enlarged data and see if we can achieve a better prediction on the validation set. we will eventually compare all the results on the test set.

```{r}
# preparing the enlarged training set, keeping subjects 14,15,16,17 for validation so that results are comparable. of course we are not touching subjects 27,28,29,30 since they will be used for testing
table(samsungData$subject)

# we are using the %in% to avoid the warning: longer object length is not a multiple of shorter object length
ltraining_subjects <- c(1,3,5,6,7,8,11,19,21,22,23,25,26)
ltraining_set <- samsung_procdata[samsung_procdata$subject %in% ltraining_subjects,]
ltraining_set <- ltraining_set[order(ltraining_set$activity),]
ltraining_set$subject <- NULL

library(tree)
mod_tree <- tree(activity ~ ., data=ltraining_set)
print(mod_tree)
```

printing the tree shows how the first split are done using a mixture of frequency, time and angle variables. this supports the idea that all variables contribute to the decision, no matter their specific domain. now check with prediction

```{r}
predvalid_tree <- predict(mod_tree, validation_set, type="class")
errvalid_tree <- sum(predvalid_tree != validation_set$activity) / length(validation_set$activity)
print(errvalid_tree)
```

using tree on larger dataset gives an error of 13%, lower than any other method used so far. now it's time to validate data on our precious test set.

```{r}

pred_randomforest <- predict(mod_randomforest, test_set)
pred_svm <- predict(mod_svm, test_set)
pred_tree <- predict(mod_tree, test_set, type="class")

err_randomforest <- sum(pred_randomforest != test_set$activity) / length(test_set$activity)
print(err_randomforest)

err_svm <- sum(pred_svm != test_set$activity) / length(test_set$activity)
print(err_svm)

err_tree <- sum(pred_tree != test_set$activity) / length(test_set$activity)
print(err_tree)

```

validating results on test set show that random forest (using bootstrap) and tree (using a larger data set) produce comparable results, with error around 10% (precisely 8% for random forest and 11% for tree), while svm in this case confirms to be the worst model with an error rate of 16%

performing a k-fold cross validation on the tree [fig. 1b] we see a significant reduction of misclassifications for a depth larger than 6, this can help creating a simplified version of the tree in order to achieve comparable error with less variables.


```{r}
# fig 1a
plot(cv.tree(mod_tree, FUN=prune.tree, method="misclass"))
mod_prunetree <- prune.tree(mod_tree,best=6)

```

we can then validate our pruned tree against the test set

```{r}
pred_prunetree <- predict(mod_prunetree, test_set, type="class")
err_prunetree <- sum(pred_prunetree != test_set$activity) / length(test_set$activity)
print(err_prunetree)
```

an error in classification of 13% is slightly higher, but the reduction of the number of variables is much greater, allowing to select the significant variables for a correct classifcation


```{r}
plot(mod_prunetree)
text(mod_prunetree)
```

conclusions
--------------------------------------------------------

the exploratory analysis revealed that a high number of variables present in these observations define values in different domains, being time, freqency and angles. further analysis reveals a clear clustering of activities around specific values of acceleration across all 3 axes [fig. 1a]

prediction defined as outcomes for activities with all the variables as covariates and using random forest based on bootstrap techniques to define the data set confirms to be comparable to ordinary tree models trained on a larger population. other kernel based regression models could achieve better results, but svm that has been used for this analysis is actually worse than trees.

```{r}
# fig 1b
plot_column <- 19
plot(training_set[,plot_column],pch=20,col=as.numeric(training_set$activity),ylab=names(samsungData)[plot_column])
```

references
--------------------------------------------------------
1. data collection description, page accessed on march 8th http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones 
2. data analysis assignment n.2, page accessed throughout the days of the assignement until march 11th https://class.coursera.org/dataanalysis-001/human_grading/index
3. Activity Recognition using Cell Phone Accelerometers, Jennifer R. Kwapisz, Gary M. Weiss, Samuel A. Moore, Department of Computer and Information Science http://www.cis.fordham.edu/wisdm/public_files/sensorKDD-2010.pdf
4. about accelerometers, gyroscopes, and relevant elements in the data set, https://class.coursera.org/dataanalysis-001/forum/thread?thread_id=2771
5. Jerk in physics, page accessed on 8th march, http://en.wikipedia.org/wiki/Jerk_(physics)
6. Elements of statistical learning http://www-stat.stanford.edu/~tibs/ElemStatLearn/