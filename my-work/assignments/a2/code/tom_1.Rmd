
```{r}
# transforming a bit the samsungData so as to cleanup names
samsung_procdata <- data.frame(samsungData, check.names=TRUE)
samsung_procdata$activity <- as.factor(samsung_procdata$activity)

# preparing the training set
training_subjects <- c(1,3,5,6)
training_set <- subset(samsung_procdata, samsung_procdata$subject==training_subjects)
training_set <- training_set[order(training_set$activity),]
training_set$subject <- NULL

factor_activity <- unique(training_set$activity)

# pick a few subjects for validation set
validation_subjects <- c(14,15,16,17)
validation_set <- subset(samsung_procdata, samsung_procdata$subject==validation_subjects)
validation_set <- validation_set[order(validation_set$activity),]
validation_set$subject <- NULL

# doing the same for test set
test_subjects <- c(27,28,29,30)
test_set <- subset(samsung_procdata, samsung_procdata$subject==test_subjects)
test_set <- test_set[order(test_set$activity),]
test_set$subject <- NULL

```

we subset the whole dataframe for training, validation and tests with the subjects we want to use, then ordering by activity, cleaning up variable names and removing the column related to subjects since we don't want it to appear in the model

we set a seed so as to generate a consistent set of training models, so that errors on validation and test will be always consistent.

```{r}
library(randomForest)
library(e1071)

set.seed(987654321)
mod_randomforest <- randomForest(activity ~ ., data=training_set, prox=TRUE)
mod_svm <- svm(activity ~ ., data=training_set)

print(mod_randomforest)
print(mod_svm)
```

random forest showing error < 4% in activity classification, now we can do some prediction on the validation test to see how the models behave.

```{r}
predvalid_randomforest <- predict(mod_randomforest, validation_set)
predvalid_svm <- predict(mod_svm, validation_set)

errvalid_randomforest <- sum(predvalid_randomforest != validation_set$activity) / length(validation_set$activity)
print(errvalid_randomforest)

errvalid_svm <- sum(predvalid_svm != validation_set$activity) / length(validation_set$activity)
print(errvalid_svm)

```

17% error using random forst and 25% using svm, we might need to tweak things a little bit to see whether we can reduce this error. maybe random forest, natively using bootstrap techniques on its training set, are overfitting.

first we enlarge the training set using a greater number of subjects, then we use normal decision trees on the enlarged data and see if we can achieve a better prediction on the validation set. we will eventually compare all the results on the test set.

```{r}
# preparing the enlarged training set, keeping subjects 14,15,16,17 for validation so that results are comparable. of course we are not touching subjects 27,28,29,30 since they will be used for testing
table(samsungData$subject)

# we are using the %in% to avoid the warning: longer object length is not a multiple of shorter object length
ltraining_subjects <- c(1,3,5,6,7,8,11,19,21,22,23,25,26)
ltraining_set <- samsung_procdata[samsung_procdata$subject %in% ltraining_subjects,]
ltraining_set <- ltraining_set[order(ltraining_set$activity),]
ltraining_set$subject <- NULL

library(tree)
mod_tree <- tree(activity ~ ., data=ltraining_set)
print(mod_tree)
```

printing the tree shows how the first split are done using a mixture of frequency, time and angle variables. this supports the idea that all variables contribute to the decision, no matter their specific domain. now check with prediction

```{r}
predvalid_tree <- predict(mod_tree, validation_set, type="class")
errvalid_tree <- sum(predvalid_tree != validation_set$activity) / length(validation_set$activity)
print(errvalid_tree)
```

using tree on larger dataset gives an error of 13%, lower than any other method used so far. now it's time to validate data on our precious test set.

```{r}

pred_randomforest <- predict(mod_randomforest, test_set)
pred_svm <- predict(mod_svm, test_set)
pred_tree <- predict(mod_tree, test_set, type="class")

err_randomforest <- sum(pred_randomforest != test_set$activity) / length(test_set$activity)
print(err_randomforest)

err_svm <- sum(pred_svm != test_set$activity) / length(test_set$activity)
print(err_svm)

err_tree <- sum(pred_tree != test_set$activity) / length(test_set$activity)
print(err_tree)

```

validating results on test set show that random forest (using bootstrap) and tree (using a larger data set) produce comparable results, with error around 10% (precisely 8% for random forest and 11% for tree), while svm in this case confirms to be the worst model with an error rate of 16%
